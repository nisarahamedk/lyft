{
 "cells": [
  {
   "source": [
    "## Custom Agent Masks for subsampling the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Reference: https://www.kaggle.com/fnands/makesubmasks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    " - Full training set is huge.\n",
    " - For prototyping the models its better to subsample the dataset to create a smaller dataset.\n",
    " - In full training dataset, frame to frame there is not much difference in a scene. if you look at framen `n` and frame `n+1` in a scene, it will look more or less identical. This much fine grained details might be required for the final model. But in order to do faster prototyping, we can go with much more coarse grained dataset.\n",
    " - But for initial prototyping, we can subsample by skipping some consecutive frames, where will have more prominant difference frame-to-frame.\n",
    "\n",
    "\n",
    " - Also matching testset.\n",
    "    - In the testset, we are asked to predict the future of the frame # 100. that is 50 future predictions.\n",
    "    - So we can subsample agents with atleast 100 history frames, and 50 future frames.\n",
    "    "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "PosixPath('/home/nisarkavungal_gmx_com/l5kit/l5kit')"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "l5kit_repo = Path(\"../../l5kit/l5kit\").resolve()\n",
    "l5kit_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, str(l5kit_repo)) # override the installed l5kit package"
   ]
  },
  {
   "source": [
    "### Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Importing fork\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from l5kit.data import LocalDataManager, ChunkedDataset\n",
    "from l5kit.dataset import AgentDataset\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from l5kit.data.filter import get_agents_slice_from_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"L5KIT_DATA_FOLDER\"] = \"/home/nisarkavungal_gmx_com/lyft-data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'/home/nisarkavungal_gmx_com/lyft-data/scenes/train.zarr'"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "dm = LocalDataManager()\n",
    "\n",
    "ds_path = dm.require(\"scenes/train.zarr\")\n",
    "ds_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<l5kit.data.zarr_dataset.ChunkedDataset at 0x7f536c29a340>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "chunked_ds = ChunkedDataset(ds_path)\n",
    "chunked_ds.open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "lyft",
   "display_name": "lyft"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}